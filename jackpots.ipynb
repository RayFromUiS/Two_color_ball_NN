{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapy_data():\n",
    "    ''' get the responses from the concated urls\n",
    "    \n",
    "    returns:\n",
    "        return the responses from the url,list\n",
    "    '''\n",
    "    \n",
    "    ##concate the url to scrape\n",
    "    responses = []\n",
    "    url_port = 'http://kaijiang.500.com/shtml/ssq/'\n",
    "    url_query = '.shtml?0_ala_baidu'\n",
    "    minimum_weeks = 149\n",
    "    from_year = '2005' ## 2 digits year\n",
    "    for year in range(int(from_year),2020):\n",
    "        for period in range(1,minimum_weeks):\n",
    "            year_str = str(year)[-2:]\n",
    "            if period < 10: ## period number <10\n",
    "                period_str = '00'+str(period)\n",
    "            elif period <100: ## period number <100\n",
    "                period_str = '0' + str(period)\n",
    "            else:            ## period number > 100\n",
    "                period_str = str(period)\n",
    "            ##concate the url\n",
    "            url = url_port + year_str + period_str + url_query\n",
    "            ## make the request\n",
    "            time.sleep(5) ## slepp for a while\n",
    "            try:\n",
    "                yield req.get(url) ## make the request and have it back as an obj\n",
    "                \n",
    "            except :\n",
    "                print('something wrong with connection')\n",
    "#         print(f'{year} has been requested')\n",
    "\n",
    "#     return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapy_data(responses):\n",
    "    '''scrapy data and yield a item\n",
    "    return\n",
    "        A well set of dictionary value container\n",
    "    '''\n",
    "    redbals = [] ## container to save the ball\n",
    "    \n",
    "    for res in responses:\n",
    "        ##decode the byte content in a manner of chinese reconginzable\n",
    "        res_text = res.content.decode('gb18030','ignore')\n",
    "        soup = BeautifulSoup(res_text,'html.parser') ## parse the text to bs object\n",
    "        ## get the red ball and blue ball\n",
    "        balls_red = soup.find_all('li',attrs = {'class': 'ball_red'})\n",
    "        for ball_red in balls_red:\n",
    "            number = ball_red.text\n",
    "            redbals.append(number)\n",
    "        ball_blue = soup.find('li',attrs = {'class':'ball_blue'})\n",
    "        ## get the date\n",
    "        date = soup.find('td',attrs ={'class':'td_title01'}) ## father tag\n",
    "        date_text = date.find('span',attrs ={'class':'span_right'}).text ## text\n",
    "        open_date = date_text.split(' ')[0].split('：')[-1].replace('年','/').replace('月','/').replace('日','')\n",
    "        close_dat = date_text.split('：')[-1].replace('年','/').replace('月','/').replace('日','')\n",
    "        ## get money spend or deposited\n",
    "        money = soup.find_all('span',attrs ={'class':'cfont1'})\n",
    "        money_spend = money[0].text.replace('元','').replace(',','') ##spended\n",
    "        money_deposit= money[1].text.replace('元','').replace(',','') ## deposited\n",
    "        \n",
    "        yield {\n",
    "            'red_balls':redbals,\n",
    "            'blue_balls':\n",
    "        }\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    '''function to scrape the previous double\n",
    "    color ball and generate a model to predict\n",
    "    the exact number\n",
    "    '''\n",
    "    url = 'http://kaijiang.500.com/shtml/ssq/18148.shtml?0_ala_baidu'\n",
    "    jakpot_number = {} ##in such a dict, peirod:{data:'',reward_num:'','deposited_from_last_period'}\n",
    "    jakpot_agg = scrapy_data(url,jakpot_number) ## scrape the data from previous data\n",
    "    jakpot_ax = plot_jakpot_agg(jakpot_agg) ## shown data with such plot\n",
    "    jakpot_train,jakpot_valid,jakpot_test = preprocess(jakpot_agg) ## in a manner of preprocess,year,month,week and day\n",
    "    jakpot_model = model_fit(jakpot_train,jakpot_valid) ## feed data to a neural network model and validated\n",
    "    jakpot_model_evlued = jakpot_model_eva(jakpot_test) ## test data on the finally model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
